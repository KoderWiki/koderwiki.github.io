---
date: 2025-11-15 11:03:00
layout: post
title: One-Class Support Vector Machine
subtitle: 'OCSVM'
description: >-
  OCSVM Novelty Detection
image: >-
  https://koderwiki.github.io/assets/img/0.post/linear/ocsvm.png
optimized_image: >-
  https://koderwiki.github.io/assets/img/0.post/linear/ocsvm.png
category: Anomaly Detection
tags:
  - OCSVM
  - Machine Learning
  - Anomaly Detection
  - Novelty Detection
  - Data Science
  - blog
author: geonu.Ko
paginate: true
use_math : true
---

## One-Class SVM

### Introduction

**One-Class Support Vector Machine (OCSVM)** 은 정상 데이터만을 이용해 정상 데이터가 분포하는 영역을 **최대한 작게 포괄하는 결정 경계 (hypersphere or hyperplnae)** 을 학습함으로써, 그 바깥에 위치하는 관측치를 이상치로 간주하는 알고리즘이다. <br>

정상 데이터만을 이용해 결정함수를 학습하는 semi-supervised model이며, 주어진 데이터의 대부분이 포함되는 최소 영역을 찾는다. <br>

그리고 정상데이터 영역 밖의 새로운 형태의 데이터는 novelty로 간주하여 novelty detection 알고리즘이다. <br>

RBF, polynomail, signoid 등의 Kernel trick을 적용함으로써 비선형 구조를 갖는 데이터에도 결정 경계 학습이 가능하며, 특히 고차원 또는 복잡한 manifold 구조의 정상 데이터를 효율적으로 감싸기 때문에 다양한 도메인에서 널리 활용 된다. <br>

### Support Vector-based Novelty Detection

SVM 원리를 이용해 이상치 탐지하는 기법이 널리 사용되고있다. <br>

label이 정상데이터만 있을때 그 데이터에 대한 boundary를 학습하고 그 boundary 밖에 데이터를 이상치로 판단하는 방법이다. <br>

대표적으로 OCSVM과 SVDD가 존재한다. <br>

### One-Class Support Vector Machine

SVM이 두 support vector 간의 margin을 최대화 시켰다면 OVSVM 은 정상데이터와 원점과의 간격을 최대화시킨다. <br>

과정은 SVM과 비슷하다. SVM에서 margin의 최대화, 올바른 분류(부등식 제약)으로 정의되었다면, OCSVM은 크게 4가지로 본다. <br>

#### 정상 데이터는 경계 안쪽으로

경계의 형태(초평면)는 다음과 같은 형태일 것이다.

$$
w^{T}\phi(x) = \rho
$$

여기서 차이점은 x가 아닌 ϕ(x)를 쓴다는 점인데, 똑같은 x이고 SVM의 특성 상 고차원일수록 경계가 복잡해져 저차원 feature space로 보내는 mapping function이다. <br>

보통 Kerner Trick을 사용해서 실 계산은 잘 하지 않는다. <br>

그리고 정상데이터는 안쪽으로, novelty 데이터는 바깥쪽이므로 다음과 같이 쓸 수 있다.

$$
w^{T}\phi(x) ≥\rho
$$

그리고 Soft-Margin SVM가 마찬가지로 제약을 완화시켜주는 slack변수 까지 두면 첫번째 목적식이 나온다.

$$
w^{T}\phi(x_i) \ge \rho - \xi_i,\qquad \xi_i \ge 0
$$

#### 변동성 최소화

hyperplane에서 w(gradient, 기울기)는 변동성을 의미한다. <br>

훈련 데이터의 변동에 따라 모델 출력의 변동 비율인데, 일반적인 함수의 기울기에 대한 정의를 생각하면 편하다. <br>

하지만 w가 크다는 것은 곧 변동성이 크다는 것을 의미하고, 이는 훈련데이터가 조금만 변해도 모델 출력이 크게 바뀌게 되어, 특정 패턴에 과도하게 반응하거나, training set에 과적합, noise에 따라가는 등, 불안정하게 된다. <br>

따라서 변동성 최소화 즉, w크기를 최소화를 시킨다.

$$
\min_{w} \ \frac{1}{2}\|w\|^{2}
$$

앞에 1/2상수는 SVM과 동일하게 Lagrange + KKT를 통해 opimization을 하는데 미분과정에서 2가 곱해져서 단순한 꼴을 위해 추가한 것이고 상수가 추가되었다고 값이 변하지는 않는다.

#### 정상데이터 영역을 원점에서 최대한 멀리 보내야한다

위 그림에서 보이다싶이, 원점과 정상데이터의 거리는 ρ로 표시한다. <br>

앞에서 언급했듯이, 일반화 성능과 더 좋은 성능을 위해 정상데이터를 원점에서 멀리 보내야 한다. <br>

따라서 다음과 같은 목적함수를 얻을수있다.