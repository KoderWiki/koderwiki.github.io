---
date: 2025-10-01 11:38:56
layout: post
title: RBF Kernel
subtitle: 'RBF Kernel'
description: >-
  RBF Kernel에 대해 알아보자
image: >-
  https://koderwiki.github.io/assets/img/0.post/linear/rbf.png
optimized_image: >-
  https://koderwiki.github.io/assets/img/0.post/linear/rbf.png
category: Machine Learning
tags:
  - SVM
  - Machine Learning
  - Classification
  - CSE
  - Data Science
  - blog
author: geonu.Ko
paginate: true
use_math : true
---

## RBF Kernel - SVM

### Introduction

비선형 데이터 분포를 다루는 문제에서 선형 분류는 한계를 지닌다. 실제데이터는 종종 입력 공간에서 선형적으로 구분되지 않으며, 이러한 경우 선형의 초평면으로는 적절한 분류를 할 수 없다. <br>

이를 해결하기 위한 대표적인 접근인 **Kernel Trick**을 통해 이 문제를 해결해 보자. <br>

### Decision Function

SVM의 결정함수는 KKT + Lagrange 를 이용해 다음과 같이 도출되었다.

$$
f(\mathbf{x}) = \sum_{i=1}^{N} \alpha_i y_i \langle \mathbf{x_i}, \mathbf{x} \rangle + b
$$

새로운 입력 x에 대해 모든 학습 샘플과의 내적을 계산하여 그 합으로 예측하는 구조이며, KKT를 통해 결국 **support vector와의 코사인유사도(Cosine similarity)** 만 확인하는 것을 확인하였다. <br>

하지만 이는 선형일때의 경우이고 비선형데이터는 단순한 초평면으로 나누기 힘들다. <br>

### Kernel Trick

**Kernel Trick**은 입력데이터를 고차원 feature space로 embedding하여, 그 공간에서 데이터를 선형적으로 구분할 수 있도록 만드는 방법이다. <br>

즉, 원래 공간에서 비선형 관계를 가진 데이터라도, 적절한 비선형 사상함수를 이용해 새로운 공간으로 옮기면 <br>

그 공간에서 다음과 같은 **선형 결정 경계(Linear Decision Boundary)** 를 적용할 수 있다.

$$
f(x) = w^T \phi(x) + b
$$

하지만 실제로 이를 명시적으로 계산할때 차원이 매우 커지기 때문에 계산 비용이 폭발적으로 증가한다. <br>

이 때문에, kernel trick은 **두 벡터의 내적을 커널 함수로 대체한다.**

$$
K(x_i,x_j)= \langle\phi(x_i), \phi(x_j)\rangle
$$

이러한 커널 함수를 사용해 결정함수를 다음과 같이 쓸 수 있다.

$$
f(\mathbf{x}) = \sum_{i=1}^{N} \alpha_i y_i K(\mathbf{x_i},\mathbf{x}) + b
$$

이 방식으로 고차원에서도 내적을 직접 계산하지 않고도 비선형 데이터를 분리할 수 있다. <br>

커널함수로는 Polynomial, sigmoid, RBF 등이 있다. <br>

이중 가장 많이 쓰이는 **RBF커널**에 대해 알아보자. <br>

### RBF Kernel

**RBF Kernel (Radial Basis Function Kernel)** 은 가장 널리 사용되는 비선형 커널이다. <br>

RBF Kernel은 두 벡터간 의 **유클리드 거리(Euclidean distance)** 를 기반으로, 입력 벡터의 유사도를 다음과 같이 계산한다.

$$
K(\mathbf{x_i}, \mathbf{x_j}) = \exp\left(-\gamma \|\mathbf{x_i} - \mathbf{x_j}\|^2\right)
$$

> · Measures similarity between two points <br>
> 
> · Close points → value near 1 <br>
> 
> · Distant points → value near 0

<br>

### Taylor Series Expansion

RBF kernel의 형태는 다음과 같이 생겼다.

$$
K(\mathbf{x_i}, \mathbf{x_j}) = \exp\left(-\gamma \|\mathbf{x_i} - \mathbf{x_j}\|^2\right)
$$

그리고 여기서 유클리드 제곱항으로 다시 써보면

$$
\|\mathbf{x_i} - \mathbf{x_j}\|^2 
= \|\mathbf{x_i}\|^2 + \|\mathbf{x_j}\|^2 - 2\mathbf{x_i}^T\mathbf{x_j}
$$

따라서 식은 다음과 같이 된다.

$$
K(\mathbf{x_i}, \mathbf{x_j}) 
= \exp\left(-\gamma(\|\mathbf{x_i}\|^2 + \|\mathbf{x_j}\|^2)\right)
\cdot \exp\left(2\gamma \mathbf{x_i}^T\mathbf{x_j}\right)
$$

<br>

이제 두 번째 항을 테일러 급수로 전개해보자. <br>

기본적으로 지수함수의 테일러급수는 다음과 같다.

$$
e^z = \sum_{n=0}^{\infty} \frac{z^n}{n!}
$$

여기서 z에 주어진 식을 대입하면

$$
\exp(2\gamma \mathbf{x_i}^T \mathbf{x_j})
= \sum_{n=0}^{\infty} \frac{(2\gamma)^n}{n!} (\mathbf{x_i}^T \mathbf{x_j})^n
$$

따라서 전체 커널은 다음과 같이 확장된다.

$$
K(\mathbf{x_i}, \mathbf{x_j})
= \exp\left(-\gamma(\|\mathbf{x_i}\|^2 + \|\mathbf{x_j}\|^2)\right)
\sum_{n=0}^{\infty} \frac{(2\gamma)^n}{n!} (\mathbf{x_i}^T \mathbf{x_j})^n
$$