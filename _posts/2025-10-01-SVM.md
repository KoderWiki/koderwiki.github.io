---
date: 2025-10-01 11:38:56
layout: post
title: Support Vector Machine
subtitle: 'SVM'
description: >-
  Support Vector Machine에 대해 알아보자
image: >-
  https://koderwiki.github.io/assets/img/0.post/linear/svm.gif
optimized_image: >-
  https://koderwiki.github.io/assets/img/0.post/linear/svm.png
category: Machine Learning
tags:
  - Machine Learning
  - Classification
  - CSE
  - Data Science
  - blog
author: geonu.Ko
paginate: true
use_math : true
---

## Support Vector Machine (SVM)

## ABSTRACT

**서포트 벡터 머신(Support Vector Machine, SVM)** 은 고차원 데이터에서도 강력한 분류 성능을 보이는 대표적인 지도학습 기법이다. <br>

Random forest, NeuralNet과 더불어 가장 많이 사용되는 ML기법으로 관련 전공자라면 한번쯤은 들어봤을법한 기법이다. <br>

SVM의 핵심 아이디어는 주어진 데이터들을 **가장 넓은 마진(margin)** 을 확보할 수 있도록 분리하는 초평면(hyperplane)을 찾는 것이다. <br>

이때 초평면과 가장 가까이 위치한 데이터 점들을 **서포트 벡터(support vectors)** 라 부르며, 이들이 결정 경계(decision boundary)를 정의하는 데 핵심적인 역할을 한다. <br>

선형적으로 분리 가능한 경우에는 최대 마진 분류기를 통해 간단히 경계를 찾을 수 있지만, 실제 데이터는 잡음(noise)과 중첩(overlap)을 포함하는 경우가 많다. <br>

이를 위해 SVM은 **소프트 마진(soft margin)** 기법과 **커널 함수(kernel trick)** 를 도입하여, 선형적으로 구분 불가능한 문제도 고차원 특징 공간(feature space)으로 사상(mapping)함으로써 효과적으로 분리할 수 있도록 확장되었다. <br>

SVM은 이론적으로 견고한 **최적화 문제(Lagrangian dual formulation)** 를 기반으로 하며, 높은 일반화 성능과 과적합 억제 능력으로 인해 텍스트 분류, 이미지 인식, 생체 데이터 분석 등 다양한 응용 분야에서 널리 활용되고 있다. <br>

<br>

## Introduction

다음과 같이 데이터를 2가지로 분류할때 어떤 직선이 더 잘 나눴는지 생각해보자.

**Example img**

대다수의 사람이 파란색 직선이라고 하겠지만, 엄밀히 말하면 둘다 잘 분류는 하고있다. <br>

하지만 파란색 직선이 더 잘 나눴다고 하는 근거는 빨간색 직선에 비해 한쪽으로 치우쳐지지 않고 적절히 잘 배치 되었기 때문이다. <br>

이걸 Machine Learning 관점에서 봤을때, train data를 잘 학습해 test data에 잘 적용되어야 한다. <br>

즉, 학습데이터는 물론, 새로운 데이터에도 잘 대응해야 하는 generalization performance가 필요하다. <br>

이러한 관점에서 봤을때, 가장 큰 틈을 가지도록 하는 선, 또는 최소거리가 최대가 되는 선이 더 좋은 선이라고 볼 수 있다. <br>

<br>

이제 이 글의 주제인 **Support Vector Machine**에 대해 알아보자. <br>

방금 생각해봤던 빨간직선, 파란 직선을 SVM에서 **결정 경계(초평면, hyperplane)** 이라 부른다. <br>

그리고 더 좋은선의 조건인 가장 큰 틈을 **margin** 이라 부르고, 위 결정 경계와 가장 가까운 데이터 포인트들을 support vector라고 부른다. <br>

즉, SVM은 이 margin을 최대화 할 수 있는 hyperplane을 찾는 알고리즘이라 할 수 있다. <br>

<br>

## Description of SVM

SVM의 핵심은 고차원에서 두개의 그룹을 가장 넓게 분리하는 가상의 초평면(hyper plane)을 찾아내는 것이다. <br>

<br>

**Figure 1. Description of SVM**

위 그림에서 각 그룹에서 가상의 구분선 까지의 거리가 가장 짧은 데이터를 **support vector** 라고 부른다. <br>

여기서 위 그림처럼 선형 데이터일 경우 직선을 그어 두가지 그룹을 분리 할 수 있지만, 현실 세계와 같은 비선형 데이터는 위 그림처럼 직선으로 나누기 쉽지 않다. <br>

하지만 **Kerner trick** 을 사용해 이런 Non-linear data 또한 아래 그림과 같이 SVM을 사용해 분류시킬 수 있다.

<br>

**Figure 2. Non-linear SVM**

이제 SVM의 원리와 다양한 종류에 대해 알아보자. <br>

<br>

## Mathematical Principles of SVM

이전 통계적 기법인 MCD는 선형대수 중심된 이론이였다면, SVM은 다변수 미적분에서 최적화 기법에 가깝다. <br>

<br>

다음과 같은 SVM이 주어졌다고 생각해보자

**그림**

이때 결정경계는 일차함수 형태로 있기 때문에, 수학적으로 표현하면 다음과 같다.

$$
w^{T}x + b = 0

$$

이를 보기쉽게 element 표현으로 적으면 다음과 같이도 표현할 수 있다. (2차원 이므로 basis 2개)

$$
w_{1}x_{1} + w_{2}x_{2} + b = 0
$$

이때 전개할 때 보기쉽게 빨간데이터 범주를 결정경계보다 +1(혹은 그 이상)이라 하고 파란 데이터 범주를 -1(혹은 그 이하)라고 해보자. <br>

그럼 범주를 다음과 같이 쓸 수 있다. <br>

빨간데이터 :

$$
w_{1}x_{1} + w_{2}x_{2} + b \geq 1
$$

파란데이터 :

$$
w_{1}x_{1} + w_{2}x_{2} + b \leq -1
$$

그럼 margin의 크기는 다음과 같이 된다.

$$
\text{Margin} = \frac{2}{\|w\|} 
= \frac{2}{\sqrt{w_1^2 + w_2^2 + \cdots + w_d^2}}
$$

위의 예시는 2차원이므로 다음과 같이 쓸 수 있다.

$$
\text{Margin} = \frac{2}{\sqrt{w_1^2 + w_2^2}}
$$

여기서 분자인 2는 우리가 경계조건을 1,-1로 놓았기때문에 나온 숫자였지만 scaling을 통해 분자가 2가 나옴을 유도할 수 있다.. <br>

**hyperplane scaling**
$$
w^{T}x + b = 0 \\
(cw)^Tx + cb = 0 \\
c(w^Tx + b) = 0 
$$
이처럼 어떠한 상수를 곱해도 초평면이 나오는 것을 알 수 있다.

그리고 margin 공식은 단순한 두 직선 사이의 거리 공식이다. <br>

**Remind: distance between two line** <br>

$$
\text{line1} = ax + by + c_{2} = 0 \\ \text{line2} = ax + by + c_{1} = 0 \\
distance : d = \frac{|c_{1} - c_{2}|}{\sqrt{w_1^2 + w_2^2}}
$$

위 공식을 경계조건을 통해 대입해 보면 다음과 같이 계산할 수 있다.

$$
d = \frac{|(b-1) - (b+1)|}{\sqrt{w_1^2 + w_2^2}} = \frac{2}{\sqrt{w_1^2 + w_2^2}}
$$

따라서 우리의 목표인 margin maximization을 위해서는 결국 분모를 최소화 시키면 되고, <br>

이는 곧 norm을 최소화 시키면 된다. <br>

SVM에서는 계산의 편의를 위해 다음을 최소화 하는 방향으로 식을 세운다.

$$
(1/2)\|w\|^2
$$

그리고 빨간데이터와 파란데이터를 동시에 만족시키는 조건은 다음과 같은 식 하나로 표현할 수 있다.

$$
y(w_{1}x_{1} + w_{2}x_{2} + b) \geq 1
$$

y에 각각 1과 -1을 넣어보면 우리가 설정한 경계조건이 나옴을 직관적으로 알 수 있다. <br>
<br>
정리해 보면, 다음과 같은 문제로 정의할 수 있다. <br>
"다음과 같은 경계조건(constraints)에서의 margin의 최대를 구하여라" <br>
이는 곧 제약조건의 최대 최소 (최적화) 문제라고 볼 수 있다. <br>
<br>
SVM의 최적화는 이런 경계조건에서의 최적화문제에서 많이 쓰이는 방법인 Lagrange multiplier method를 사용해서 해결한다. <br>
<br>

라그랑즈 승수법에 대한 자세한 내용은 다음 post에 담겨있다. <br>

[**Lagrange multiplier method**](https://kodernote.github.io/cse,/ai,/calculus,/optimization/2025/08/29/lagrange.html)

간략하게 설명하면 라그랑즈 승수법은 함수의 등고면과 제약곡면이 접할때 <br>

즉 **gradient**가 **평행** 할때 **최대 최소**를 갖는 것을 의미한다. <br>

$$
∇f(x,y,z) = λ∇g(x,y,z)
$$

<br>

이를 우리가 풀어야 할 최적화 문제로 문제정의를 하면 다음과 같이 쓸 수 있다

$$
L(w, b, \alpha) = \frac{1}{2}\|w\|^2 - \sum_i \alpha_i \big[ y_i (w^T x_i + b) - 1 \big]

$$

일단 해석을 해보면 앞에 1/2 norm(w)^2은 margin의 최대화 즉, 목적함수 이다. <br>

그리고 뒤 두번째 항은 constraints condition을 만족하는 정도이고 α는 라그랑즈 상수, <br>

원래의 형태에서 lambda에 해당한다. <br>

그럼 sigma는 뭐냐, <br>

SVM 제약조건은 각 데이터마다 존재하기 때문에 하나씩 붙여서 더해주어야 한다. <br>

그러면 여기서 support vector의 제약조건만 계산하면 되는데 왜 모든 데이터에 다 정의하는지에 대한 의문을 가질 수 있다. <br>

이유는 처음에는 누가 support vector인지 모르기 때문에, 모든 데이터에 대해 constraints를 걸어야 한다. <br>

<br>

그리고 이후에 이 알고리즘에 핵심인 Karush-Kuhn-Tucker (KKT) conditions을 사용한다. <br>

KKT 조건은 다음과 같다 <br>

① **Stationarity** <br>

② **Primal feasibility** <br>

③ **Dual feasibility** <br>

④ **Complementray slackness** <br>

<br>

원래 풀던대로 부등식에서 최대 최소는 경계조건에서의 최대 최소값과 내부 점에서 최대 최소값을 찾아서 비교하면 되지 않을까? 하는 의문을 가질 수 있지만, <br>

단순한 작은 문제에서는 쉬울 수 있지만, 일반 최적화에서는 KKT 조건을 사용했을때 경계와 내부를 더 체계적이고 자동화 된 방식으로 풀 수 있다. <br>

<br>

다시 본론으로 돌아와 KKT조건을 이용해 해결해 보자 <br>
