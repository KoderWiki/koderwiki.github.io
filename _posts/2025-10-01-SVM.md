---
date: 2025-10-01 11:38:56
layout: post
title: Support Vector Machine
subtitle: 'SVM'
description: >-
  Support Vector Machine에 대해 알아보자
image: >-
  https://koderwiki.github.io/assets/img/0.post/linear/svm.gif
optimized_image: >-
  https://koderwiki.github.io/assets/img/0.post/linear/svm.png
category: Machine Learning
tags:
  - Machine Learning
  - Classification
  - CSE
  - Data Science
  - blog
author: geonu.Ko
paginate: true
use_math : true
---

## Support Vector Machine (SVM)

## ABSTRACT

**서포트 벡터 머신(Support Vector Machine, SVM)** 은 고차원 데이터에서도 강력한 분류 성능을 보이는 대표적인 지도학습 기법이다. <br>

Random forest, NeuralNet과 더불어 가장 많이 사용되는 ML기법으로 관련 전공자라면 한번쯤은 들어봤을법한 기법이다. <br>

SVM의 핵심 아이디어는 주어진 데이터들을 **가장 넓은 마진(margin)** 을 확보할 수 있도록 분리하는 초평면(hyperplane)을 찾는 것이다. <br>

이때 초평면과 가장 가까이 위치한 데이터 점들을 **서포트 벡터(support vectors)** 라 부르며, 이들이 결정 경계(decision boundary)를 정의하는 데 핵심적인 역할을 한다. <br>

선형적으로 분리 가능한 경우에는 최대 마진 분류기를 통해 간단히 경계를 찾을 수 있지만, 실제 데이터는 잡음(noise)과 중첩(overlap)을 포함하는 경우가 많다. <br>

이를 위해 SVM은 **소프트 마진(soft margin)** 기법과 **커널 함수(kernel trick)** 를 도입하여, 선형적으로 구분 불가능한 문제도 고차원 특징 공간(feature space)으로 사상(mapping)함으로써 효과적으로 분리할 수 있도록 확장되었다. <br>

SVM은 이론적으로 견고한 **최적화 문제(Lagrangian dual formulation)** 를 기반으로 하며, 높은 일반화 성능과 과적합 억제 능력으로 인해 텍스트 분류, 이미지 인식, 생체 데이터 분석 등 다양한 응용 분야에서 널리 활용되고 있다. <br>

<br>

## Introduction

다음과 같이 데이터를 2가지로 분류할때 어떤 직선이 더 잘 나눴는지 생각해보자.

**Example img**

대다수의 사람이 파란색 직선이라고 하겠지만, 엄밀히 말하면 둘다 잘 분류는 하고있다. <br>

하지만 파란색 직선이 더 잘 나눴다고 하는 근거는 빨간색 직선에 비해 한쪽으로 치우쳐지지 않고 적절히 잘 배치 되었기 때문이다. <br>

이걸 Machine Learning 관점에서 봤을때, train data를 잘 학습해 test data에 잘 적용되어야 한다. <br>

즉, 학습데이터는 물론, 새로운 데이터에도 잘 대응해야 하는 generalization performance가 필요하다. <br>

이러한 관점에서 봤을때, 가장 큰 틈을 가지도록 하는 선, 또는 최소거리가 최대가 되는 선이 더 좋은 선이라고 볼 수 있다. <br>

<br>

이제 이 글의 주제인 **Support Vector Machine**에 대해 알아보자. <br>

방금 생각해봤던 빨간직선, 파란 직선을 SVM에서 **결정 경계(초평면, hyperplane)** 이라 부른다. <br>

그리고 더 좋은선의 조건인 가장 큰 틈을 **margin** 이라 부르고, 위 결정 경계와 가장 가까운 데이터 포인트들을 support vector라고 부른다. <br>

즉, SVM은 이 margin을 최대화 할 수 있는 hyperplane을 찾는 알고리즘이라 할 수 있다. <br>

<br>

## Description of SVM

SVM의 핵심은 고차원에서 두개의 그룹을 가장 넓게 분리하는 가상의 초평면(hyper plane)을 찾아내는 것이다. <br>

<br>

**Figure 1. Description of SVM**

위 그림에서 각 그룹에서 가상의 구분선 까지의 거리가 가장 짧은 데이터를 **support vector** 라고 부른다. <br>

여기서 위 그림처럼 선형 데이터일 경우 직선을 그어 두가지 그룹을 분리 할 수 있지만, 현실 세계와 같은 비선형 데이터는 위 그림처럼 직선으로 나누기 쉽지 않다. <br>

하지만 **Kerner trick** 을 사용해 이런 Non-linear data 또한 아래 그림과 같이 SVM을 사용해 분류시킬 수 있다.

<br>

**Figure 2. Non-linear SVM**

이제 SVM의 원리와 다양한 종류에 대해 알아보자. <br>

<br>

## Mathematical Principles of SVM

이전 통계적 기법인 MCD는 선형대수 중심된 이론이였다면, SVM은 다변수 미적분에서 최적화 기법에 가깝다. <br>

<br>

다음과 같은 SVM이 주어졌다고 생각해보자

**그림**

이때 결정경계는 일차함수 형태로 있기 때문에, 수학적으로 표현하면 다음과 같다.

$$
w^{T}x + b = 0

$$

이를 보기쉽게 element 표현으로 적으면 다음과 같이도 표현할 수 있다. (2차원 이므로 basis 2개)

$$
w_{1}x_{1} + w_{2}x_{2} + b = 0
$$

이때 전개할 때 보기쉽게 빨간데이터 범주를 결정경계보다 +1(혹은 그 이상)이라 하고 파란 데이터 범주를 -1(혹은 그 이하)라고 해보자. <br>

그럼 범주를 다음과 같이 쓸 수 있다. <br>

빨간데이터 :

$$
w_{1}x_{1} + w_{2}x_{2} + b \geq 1
$$

파란데이터 :

$$
w_{1}x_{1} + w_{2}x_{2} + b \leq -1
$$

그럼 margin의 크기는 다음과 같이 된다.

$$
\text{Margin} = \frac{2}{\|w\|} 
= \frac{2}{\sqrt{w_1^2 + w_2^2 + \cdots + w_d^2}}
$$

위의 예시는 2차원이므로 다음과 같이 쓸 수 있다.

$$
\text{Margin} = \frac{2}{\sqrt{w_1^2 + w_2^2}}
$$

여기서 분자인 2는 우리가 경계조건을 1,-1로 놓았기때문에 나온 숫자임을 인지하자. <br>

일반화 형태
$$
w_{1}x_{1} + w_{2}x_{2} + b \geq k \\
w_{1}x_{1} + w_{2}x_{2} + b \leq -k \\
\text{Margin} = \frac{2k}{\|w\|} 
= \frac{2k}{\sqrt{w_1^2 + w_2^2 + \cdots + w_d^2}}
$$

그리고 margin 공식은 단순한 두 직선 사이의 거리 공식이다. <br>

**Remind: distance between two line** <br>

$$
\text{line1} = ax + by + c_{2} = 0 \\ \text{line2} = ax + by + c_{1} = 0 \\
distance : d = \frac{|c_{1} - c_{2}|}{\sqrt{w_1^2 + w_2^2}}
$$

위 공식을 경계조건을 통해 대입해 보면 다음과 같이 계산할 수 있다.

$$
d = \frac{|(b-1) - (b+1)|}{\sqrt{w_1^2 + w_2^2}} = \frac{2}{\sqrt{w_1^2 + w_2^2}}
$$

따라서 우리의 목표인 margin maximization을 위해서는 결국 분모를 최소화 시키면 되고, <br>

이는 곧 norm을 최소화 시키면 된다. <br>

SVM에서는 계산의 편의를 위해 다음을 최소화 하는 방향으로 식을 세운다.

$$
(1/2)\|w\|^2
$$

그리고 빨간데이터와 파란데이터를 동시에 만족시키는 조건은 다음과 같은 식 하나로 표현할 수 있다.

$$
y(w_{1}x_{1} + w_{2}x_{2} + b) \geq 1
$$