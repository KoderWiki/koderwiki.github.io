---
date: 2025-10-01 11:38:56
layout: post
title: Support Vector Machine
subtitle: 'SVM'
description: >-
  Support Vector Machine에 대해 알아보자
image: >-
  https://koderwiki.github.io/assets/img/0.post/linear/svm.gif
optimized_image: >-
  https://koderwiki.github.io/assets/img/0.post/linear/svm.png
category: Machine Learning
tags:
  - Machine Learning
  - Classification
  - CSE
  - Data Science
  - blog
author: geonu.Ko
paginate: true
use_math : true
---

## Support Vector Machine (SVM)

## ABSTRACT

**서포트 벡터 머신(Support Vector Machine, SVM)** 은 고차원 데이터에서도 강력한 분류 성능을 보이는 대표적인 지도학습 기법이다. <br>

Random forest, NeuralNet과 더불어 가장 많이 사용되는 ML기법으로 관련 전공자라면 한번쯤은 들어봤을법한 기법이다. <br>

SVM의 핵심 아이디어는 주어진 데이터들을 **가장 넓은 마진(margin)** 을 확보할 수 있도록 분리하는 초평면(hyperplane)을 찾는 것이다. <br>

이때 초평면과 가장 가까이 위치한 데이터 점들을 **서포트 벡터(support vectors)** 라 부르며, 이들이 결정 경계(decision boundary)를 정의하는 데 핵심적인 역할을 한다. <br>

선형적으로 분리 가능한 경우에는 최대 마진 분류기를 통해 간단히 경계를 찾을 수 있지만, 실제 데이터는 잡음(noise)과 중첩(overlap)을 포함하는 경우가 많다. <br>

이를 위해 SVM은 **소프트 마진(soft margin)** 기법과 **커널 함수(kernel trick)** 를 도입하여, 선형적으로 구분 불가능한 문제도 고차원 특징 공간(feature space)으로 사상(mapping)함으로써 효과적으로 분리할 수 있도록 확장되었다. <br>

SVM은 이론적으로 견고한 **최적화 문제(Lagrangian dual formulation)** 를 기반으로 하며, 높은 일반화 성능과 과적합 억제 능력으로 인해 텍스트 분류, 이미지 인식, 생체 데이터 분석 등 다양한 응용 분야에서 널리 활용되고 있다. <br>

<br>

## Introduction

다음과 같이 데이터를 2가지로 분류할때 어떤 직선이 더 잘 나눴는지 생각해보자.

**Example img**

대다수의 사람이 파란색 직선이라고 하겠지만, 엄밀히 말하면 둘다 잘 분류는 하고있다. <br>

하지만 파란색 직선이 더 잘 나눴다고 하는 근거는 빨간색 직선에 비해 한쪽으로 치우쳐지지 않고 적절히 잘 배치 되었기 때문이다. <br>

이걸 Machine Learning 관점에서 봤을때, train data를 잘 학습해 test data에 잘 적용되어야 한다. <br>

즉, 학습데이터는 물론, 새로운 데이터에도 잘 대응해야 하는 generalization performance가 필요하다. <br>

이러한 관점에서 봤을때, 가장 큰 틈을 가지도록 하는 선, 또는 최소거리가 최대가 되는 선이 더 좋은 선이라고 볼 수 있다. <br>

<br>

이제 이 글의 주제인 **Support Vector Machine**에 대해 알아보자. <br>

방금 생각해봤던 빨간직선, 파란 직선을 SVM에서 **결정 경계(초평면, hyperplane)** 이라 부른다. <br>

그리고 더 좋은선의 조건인 가장 큰 틈을 **margin** 이라 부르고, 위 결정 경계와 가장 가까운 데이터 포인트들을 support vector라고 부른다. <br>

즉, SVM은 이 margin을 최대화 할 수 있는 hyperplane을 찾는 알고리즘이라 할 수 있다. <br>

<br>

## Description of SVM

SVM의 핵심은 고차원에서 두개의 그룹을 가장 넓게 분리하는 가상의 초평면(hyper plane)을 찾아내는 것이다. <br>

<br>

**Figure 1. Description of SVM**

위 그림에서 각 그룹에서 가상의 구분선 까지의 거리가 가장 짧은 데이터를 **support vector** 라고 부른다. <br>

여기서 위 그림처럼 선형 데이터일 경우 직선을 그어 두가지 그룹을 분리 할 수 있지만, 현실 세계와 같은 비선형 데이터는 위 그림처럼 직선으로 나누기 쉽지 않다. <br>

하지만 **Kerner trick** 을 사용해 이런 Non-linear data 또한 아래 그림과 같이 SVM을 사용해 분류시킬 수 있다.

<br>

**Figure 2. Non-linear SVM**

이제 SVM의 원리와 다양한 종류에 대해 알아보자. <br>

<br>

##