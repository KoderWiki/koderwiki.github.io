---
layout: post

title: Thesis review, Minimum Covariance Determinant

tags: [Linear Algebra, CSE, Data Science, AI, Statistics]

feature-img: "assets/img/0.post/linear/2.png"

thumbnail: "assets/img/0.post/linear/2.png"

categories: CSE, AI
---

## THESIS REVIEW: Minimum Covariance Determinant

이 글은 [**Hubert, M., & Debruyne, M. (2010). Minimum covariance determinant.**](https://wis.kuleuven.be/stat/robust/papers/2010/wire-mcd.pdf)

논문을 토대로 작성되었습니다. <br>

### 1. ABSTRACT

**MCD(Minimum Covariance Determinant)방법**은 빠른 알고리즘을 통해 **다변량 위치(location)** 와 **산포(scatter)** 의 **강건한(robust) 추정치**를 제공하는 기법이다. <br>

**공분산 행렬(Covariance matrix)** 는 다양한 통계방법의 초석이 되므로, MCD는 강건하고 계산적으로 효율적인 다변량 기법들을 개발하는데도 사용되고 있다. <br>

더 나아가 MCD는 **이상치(outlier) 감지를 위한 효율적이고 실용적인 도구**로 활용될 수 있어, 데이터 분석의 신뢰성을 높이는 데 기여한다. <br>

<br>

**MCD estimator**는 **affine equivariance**, **breakdown value**, 그리고 **influence function**의 주요 속성에 대한 내용과 <br>

affine equivariant이면서 robust를 상속하는 **빠른 결정론적 알고리즘**과 차원의 수가 case보다 많은 수 있는 고차원에 대해 설계되고 특이 행렬을 방지하기 위한 **정규화**에 대한 최근 확장 개념이 묘사되어있다. <br>

MCD는 의학, 금융, cv등 다양한 분야에 적용되고, **PCA(주성분분석)**, **Regression(회귀분석)**, **Factor analysis(요인분석)** 등 다변량 기술을 개발하는데도 사용되고 있다.

우선 MCD를 사용해야 하는 이유에 대한 예시를 살펴보자.

### 2. DESCRIPTION OF THE MCD ESTIMATOR

다변량 위치와 산포의 세팅에서, 데이터는 nxp 데이터 행렬에 저장되어있다고 가정한다.

$$
X=(x1​,…,xn​)⊤
$$

여기서 각 xi는 i번째 **관측값(i-th observation**이다.<br>

따라서 **n은 객체(object)의 개수**, **p는 변수(variable)의 개수**를 의미한다.

<br>

**Figure 1, Bivariate wine data with classical and robust tolerance ellipse**

<img width="738" height="584" alt="image" src="https://github.com/user-attachments/assets/26306fc1-00c0-45fb-bc23-43da31506fb7" />


<br>

예시를 위해 **이변량(bivariate, p=2) 데이터 집합**을 고려한다.<br>

이 논문에서는 와인 데이터셋을 고려하는데 이 데이터셋은 3종류의 이탈리아 와인에서 발견된 13가지 성분의 양을 포함되어 있고, 첫번째 그룹에 속한 59개의 와인중 **사과산(malic acid)**, **프롤린(proline)** 성분에 집중하고 있다. <br>

위 **Figure 1**에 그 데이터의 산점도(scatter plot)이 제시되어있고, **classical** 및 **rubust** 97.5% **허용타원(tolerance ellipse)** 가 함께 그려져있다.

<br>

#### 2.1 Mahalanobis distance (마할라노비스 거리)

여기서 잠시 mahalanobis distance의 개념에 대해 짚고 넘어가자. <br>

공식은 다음과 같다.

$$
MD(x) = \sqrt{(x - \bar{x})^{\top} S^{-1} (x - \bar{x})}
$$

우선 공식을 이해하기 위해서는 **이차형식(quadratic form)** 에 대한 개념을 알고 있어야 한다. <br> 공식 유도과정은 다음과 같다.

$$
\textbf{가정: }\;\Sigma \in \mathbb{R}^{p \times p}\text{ 는 대칭 양의정부호(SPD) 공분산 행렬, }\;\mu \in \mathbb{R}^p
$$

$$
\Sigma = U \Lambda U^\top,\quad
\Lambda = \mathrm{diag}(\lambda_1,\dots,\lambda_p),\;\lambda_i>0
$$

$$
\Sigma^{-1/2} = U \Lambda^{-1/2} U^\top,\quad
\Lambda^{-1/2}=\mathrm{diag}(\lambda_1^{-1/2},\dots,\lambda_p^{-1/2})
$$

$$
z := \Sigma^{-1/2}(x-\mu)
$$

$$
\|z\|_2^2
= (x-\mu)^\top \big(\Sigma^{-1/2}\big)^\top \Sigma^{-1/2} (x-\mu)
$$

$$
= (x-\mu)^\top \Sigma^{-1} (x-\mu)
$$

$$
\therefore\; MD(x) = \sqrt{(x-\mu)^\top \Sigma^{-1} (x-\mu)}
$$

쉽게 설명하자면 거리는 절대값이기때문에 루트에 제곱을 씌워 다음과같은 형태임을 알수있다.

$$
\sqrt{(x - \mu)^2}
$$

이는 내적의 정의에 따라 다음과 같다

$$
\sqrt{(x-\mu)^\top (x-\mu)}
$$

여기서 루트안에 있는 다음의 식을 **이차형식**으로 생각하면 반지름이 1인 원이 나온다<br>

하지만 거리를 계산할때 **분산(variance)** 의 존재때문에 (퍼짐의정도) 거리가 객관적으로 나오지 않고 밀도가 높은 곳은 거리가 덜반영되고, 낮은 곳은 더 크게 반영되는 문제가 있다. <br>

따라서 공분산으로 나눠 밀도를 맞춰주고 이를 **whitening**이라고 한다. <br>

그리고 이 과정을 통해 **고유치(eigen value)의 역수** 를 계수로 삼는 **타원(ellipse)** 이 나온다. <br>

이 과정을 생각하면 다음의 공식으로 바로 유도할 수 있다

$$
\therefore\; MD(x) = \sqrt{(x-\mu)^\top \Sigma^{-1} (x-\mu)}
$$

<br>

이 마할라노비스거리는 점 x가 data cloud의 중심으로 부터, 그 크기에 비해 **얼마나 멀리 떨어져 있는지**를 알려준다.

**참고** <br>

확률분포에서 어느 방향으로의 퍼진정도가 **분산(variance)** 이고 이 분산은 다시 말해 **고유치(eigen value)** 가 된다.<br>

그리고 그 방향은 **고유벡터(eigen vector)** 가 된다.

<br>

#### 2.2 Robust Distance

그럼 본론으로 돌아와서 **classical**과 **robust** 의 **tolerance ellipse**를 비교해보자. <br>

**Classical tolerance ellipse**는 MD(x)가 **카이제곱분포**의 분위수가 같아지는 p-차원 점 x들의 집합으로 정의한다.

밑 **Figure 2**는 **Mahalanobis distance**와 **robust distance**를 비교한 그림이다.

<br>

**Figure 2**

<img width="1570" height="608" alt="image" src="https://github.com/user-attachments/assets/81ac5339-3ad6-4ebc-9721-fedee473616c" />


<br>

Figure 2에서 보이다 싶이 기본 마할라노비스 거리의 타원은 모든 관측치를 포괄하려는 것을 알 수 있다. 따라서 이상치가 3개밖에 안보이는 것을 알 수 있다.<br>

반면에 Figure1의 **robust tolerance ellipse**는 다음식을 따르고 더 작고 정규화된 데이터 부분만 포함하고 있다.

$$
RD(x) = \sqrt{(x - \hat{\mu}_{\text{MCD}})^\top \hat{\Sigma}_{\text{MCD}}^{-1} (x - \hat{\mu}_{\text{MCD}})}
$$

여기서 μ_MCD는 위치의 mcd추정량이고, Σ_MCD는 mcd의 공분산 추정치 이다. <br>

이롤 이용하면 Figure 2(b)처럼 8개의 이상치와 1개의 이상치를 찾아내는 것을 볼 수 있다. <br>

**classical estimates**가 **이상치(outlying values)** 의 영향을 너무 크게 받아, MD(x)와 같은 확인 수단들이 더이상 이상치를 탐지할 수 없게 되는 현상을 **masking effect**라고 한다. <br>

이런 데이터의 신뢰할 수 있는 분석을 위해 **robust estimator**가 필요하다.

<br>

### 3. Definition

우선 [.]은 **바닥함수(floor function)** 을 나타낸다. <br>

x를 넘지않는 최대의 정수를 뜻하는 것으로 흔히 아는 **Gauss symbol**이다. <br>

매개변수 **[(n + p + 1)/2] ≤ h ≤ n** 갖는 **raw MCD 추정량**은 다음과 같은 **위치(location)** 와 **산포(dispersion)** 추정치를 정의한다. <br>

> μ_0은 표본 공분산 행렬의 행렬식이 최소가 되는 h개의 관측치들의 평균이다 <br>
> 
> Σ는 공분산행렬에 consistency factor(보정계수) **c**를 곱한 것이다.

<br>

여기서 MCD 추정량은 **h>p**일때만 계산이 가능한데, 그렇지 않으면 어떤 h의 subset의 공분산행렬이 **특이(singular)** 하게 된다. <br>

이유에 대해 잠시 알아보고 넘어가자

<br>

<br>

#### 3-1 Invertibility Condition

**MCD의 목적은 이상치에 둔감한(robust) 공분산 추정치를 만드는 것이다**. <br>

때문에 n개의 데이터중 **h개의 관측치(이상치가 덜 포함된 subset)** 을 찾아서 평균, 공분산을 계산한다. <br>

여기서 h개의 subset을 추출했을때 나오는 데이터 행렬은 **hxp** 행렬일 것이다.<br>

$$
X =
\begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1p} \\
x_{21} & x_{22} & \cdots & x_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
x_{h1} & x_{h2} & \cdots & x_{hp}
\end{bmatrix}
\in \mathbb{R}^{h \times p}
$$

여기서 공분산 행렬을 구하면 다음과 같다.

$$
\hat{\Sigma}_{MCD} = \frac{1}{h} \sum_{i=1}^{h} (x_i - \bar{x}_h)(x_i - \bar{x}_h)^\top
$$

공식의 의미를 부가설명하자면 이상치가 덜 포함된 subset을 추출했기때문에 원래의 평균으로 빼버리면 이상치로 왜곡된 평균을 빼버리는것이기 때문에 MCD의 강건성이 떨어진다. <br>

따라서 subset의 평균을 구하고 그걸로 다시 빼서 평균을 구한다. 그리고 이걸 **중심화**라고 한다.<br>

이때 중심화로 인해 열의 합이 0이 되서 **종속**이 된다. 따라서 정확한 조건은<br>

**h-1 > p** 가 더 정확하다. 그리고 그 종속이 되는 한 행은 **표본평균**이다.<br>

<br>

조건에 대해 설명하기 전에 다음을 먼저 기억하자. <br>

> rank(Σ) < dim => 비가역(invertible) <br>
> 
> 따라서 rank(Σ) = p여야 가역이다. (pxp 행렬)

여기서 h-1>p인 이유는 공분산 행렬에서 (x - x바)를 Z_h라고 할때 <br>

$$
Z_h \;=\; X - \mathbf{1}_h\,\bar{x}_h^{\top}
\qquad(\bar{x}_h=\tfrac{1}{h}\sum_{i=1}^h x_i)
$$

$$
\text{(일반위치 가정 하에)}\quad
1 \;\le\; \operatorname{rank}(Z_h) \;\le\; \min\!\big(p,\;h-1\big)
$$

원래의 조건이면 0 <= rank(A) 겠지만 Z는 영행렬이 아니므로 1보다 커야한다. <br>

여기서 만약 h-1 < p라고 가정해보자. <br>

그럼 최대 rank가 h-1이기 때문에 공분산행렬 계산결과 또한 h-1 x h-1 행렬이 나올것이다. <br>

근데 여기서 h-1 < p 이므로 rank(Σ) <= h-1 < p = dim이 되어 **비가역**이 된다. <br>

따라서 **h-1 > p** 라는 조건이 필요하다.

<br>
<br>

#### 3-2 elliptically symmetric unimodal distributions

MCD 추정량은 타원대칭(elliptically symmetric)이며, unimodal한 분포를 위해 설계되었다. <br>

매개변수가 μ ∈ R^p 이고, Σ가 크기 p의 **양의 정부호 행렬**인 다변량 분포가 다음 조건을 만족하면 타원 대칭이고 **단봉(unimodal)** 이라 한다

어떤 **엄격히 감소하는 함수 g** 가 존재하여, 밀도가 다음과 같은 형태로 쓸 수 있을 때이다.

$$
f(x) \;=\; \frac{1}{\sqrt{|\Sigma|}} \; g\!\left((x - \mu)^\top \Sigma^{-1} (x - \mu)\right)
$$

우선 **양정치행렬**을 가져야 하는이유는 **이차형식**이 **양수**여야 거리처럼 쓸 수 있기 때문이다.<br>

그리고 앞에 det(Σ)를 제곱근으로 나눠주는이유는 확률밀도함수의 적분 총합은 1이여야 하기 때문이다. <br>

여기서 선형변환의 넓이에 대한 이해가 있어야 한다.<br>

간단히 설명하자면, 선형변환 T에 대한 S가 T의 정의역에 해당하는 부분집합이면 T(S)의 넓이는 다음과 같다.

$$
T(S)의 \ 넓이 \ = \ |detA| \times S의 넓이
$$

**Jacobian matrix**를 알고있다면 이해하기 쉽다. <br>

우선 단위구로 하면 편하겠지만 분산이 1이 아닌 방향마다 다른 고유치가 있기때문에 다음 선형변환을 통해 타원으로 맞춰준다. <br>

$$
y \;=\; \Sigma^{-\tfrac{1}{2}} (x - \mu)
$$

위 선형변환을 통해 넓이는 det(Σ)제곱근에 비례하게 되고 이를 1로 맞추기 위해 나눠준다.

<br>
<br>

#### 3-3 Reweighted Estimator

MCD 추정에서 h의 의미는 전체표본 n개 중에서 이상치 없는 h개의 관측치를 골라서 subset를 형성해서 **location**과 **scatter**를 계산하는 것이다. <br>

여기서 h값이 작을수록 많은 이상치(outlier)를 버릴수 있어 높은 **breakdown point**를 얻지만 많이 버린다는 것은 활용량이 줄어든다는 것이다. <br>

<br>

추후 언급하겠지만 **breakdown point**는 **데이터 중 일부를 이상치로 두었을때 추정량이 망가지는 최소 비율**이다.<br>

예를들어 **평균**은 한개의 이상치만 추가되도 확 변할 수 있기 때문에 breakdown value가 낮다고 볼 수 있다. <br>

하지만 **중앙값(median)** 은 절반 이상을 이상치로 바꾸는 거 아니면 변하지 않는다. 따라서 breakdown value는 50이다.

<br>

MCD추정량은 h = [n+p+1]/2 에서 가장 robust하다. 이는 모집단 수준에서 대략 절반에 해당한다. <br>

하지만 이 경우 효율성이 낮다는 단점이 있다. <br>

예를 들어 변수가 2개 일때 점근상대효율은 공분산행렬에 비해 6퍼 밖에 안되고 10개일 때는 20.5밖에 되지 않는다. <br>

때문에 효율성을 위해 더 큰 값을 고려할 수도 있겠지만 이러면 **견고성**을 감소시킨다.

<br>

때문에 효율성을 높이면서 높은 견고성을 위해 **재가중치 추정량(reweighted estimator)** 을 사용한다.

$$
\hat{\mu}_{MCD} \;=\; \frac{\sum_{i=1}^n W(d_i^2) x_i}{\sum_{i=1}^n W(d_i^2)}
$$

$$
d_i \;=\; \sqrt{(x - \hat{\mu}_0)^t \,\hat{\Sigma}_0^{-1}\,(x - \hat{\mu}_0)}

$$

$$
\hat{\Sigma}_{MCD} \;=\; c_1 \frac{1}{n} \sum_{i=1}^n W(d_i^2)\,(x_i - \hat{\mu}_{MCD})(x_i - \hat{\mu}_{MCD})^t
$$

수식에 대해 설명하자면

> W(.) : 가중함수, d: 마할라노비스 거리<br>
> 
> 분모: 가중치 총합, 분자: 가중치의 합
> 
> c : 일치성 계수(consistency factor)

여기서 마할라노비스에 있는 공분산은 **초기 MCD**이고 재가중치 mcd가 아니다.<br>

분자에서 점에 대한 이상치일수록 W가 0이 되어 반영안되고 정상치일수록 1에 가까워저 많이 반영된다. <br>

W에 대해 간단하면서 효과적인 선텍은 다음과같다

$$
W(d^2) = I \big( d^2 \leq \chi^2_{p,0.975} \big)
$$

그리고 이는 p=2일때 최대 45.5%, p=10에서는 82%까지 증가시킨다.<br>

<br>

### 4. Outlier detection

Robust MCD 추정량은 이상치를 탐지하는데 매우 유용하고, Robust distacne는 masking effect에 민감하지 않아서 이상치를 표시(flag)하는데 사용될 수 있다. <br>

이는 2차원(또는 3차원) 이상의 데이터셋, 즉 시각화하기 어려운 경우에서 훨씬 더 유용해진다. <br>

Robust Distance는 μ,Σ를 알고있다면, 이론적으로 카이제곱 분포를 따른다는 것이다.<br>

하지만 현실에서는 μ,Σ를 알수가 없기때문에 **표본평균**과 **표본 공분산**을 사용해야 하고, <br>

추정오차때문에 **F분포**에 근사된다. <br>

<br>

### 5. Properties

#### 5-1 Affine Equivariance

MCD 추정량(location, scatter)은 **affine equivariance**를 가진다. <br>

이는 임의의 가역행렬 A와 상수벡터 b에 대해 다음이 성립한다는 의미이다.

$$
\hat{\mu}_{MCD}(AX + b) = A \hat{\mu}_{MCD}(X) + b
$$

$$
\hat{\Sigma}_{MCD}(AX + b) = A \hat{\Sigma}_{MCD}(X) A^t
$$

첫번째는 선형성을 의미하고 두번째는 축변환해도 동일하다는 것을 의미한다. <br>

즉 **변환규칙**을 따라간다.<br>

이 성질은, 크기가 h인 각 부분집합에 대해, 변환된 데이터의 공분산 행렬의 행렬식이 다음의 수식이 성립한다는 것에서 바로 도출된다.

$$
|S(AX_h)| = |A S(X_h) A^t| = |A|^2 |S(X_h)|
$$

따라서 최소화하는 최적의 h부분집합은 원레 데이터 최소화하는 부분집합과 **동일하게 유지**되고, 그 공분산 행렬은 적절하게 변환된다. <br>

Robust distance도 역시 아핀불변이므로, 재가중치된 추정량도 아핀 동치성을 갖는다.

즉 평행이동을 하든, 어떤 아핀변환을 하든 이상치탐지에는 영향을 안준다.<br>

<br>

#### 5-2 Breakdown Value

Breakdown value는 앞에서 언급했듯이, 추정값을 무한히 벗어나게 만들기 위해 임의의 값으로 대체해야하는 관측치의 **최소 비율**이다. <br>

**multivariate location 추정량의 breakdwon value**는 다음과 같이 정의된다.

$$
\epsilon_n^{*}(T_n; X_n) = \frac{1}{n} \min \left\{ m \in \{1, \ldots, n\} : \sup_m \|T_n(X_n) - T_n(X_{n,m})\| = +\infty \right\}.
$$

여기서 수식을 설명하자면, <br>

> n : 전체데이터, m : 대체데이터개수(1~n) <br>
> 
> Xn,m : n개중 m개를 바꿈 <br>
> 
> Tn : 추정량(평균, 분산 등), norm : 거리 <br>

즉, n개중 m개의 값을 바꿀때 무한히 크거나 작게 만드는 m의 최소값이고 이를 n으로 나눠 평균을 구하는 것이다.<br>

<br>

그리고 multicariate scatter(다변량 산포) 추정량의 경우에는

$$
\epsilon_n^{*}(C_n; X_n) = \frac{1}{n} \min \left\{ m \in \{1, \ldots, n\} : \sup_m \max_i \left| \log(\lambda_i(C_n(X_n))) - \log(\lambda_i(C_n(X_{n,m}))) \right| \right\},
$$

> C(.) : 산포 추정량(예: 공분상 행렬 추정치)<br>
> 
> λi(Cn(Xn)) : i-th eigen value <br>
> 
> Xnm : 원래 데이터 X에서 m개의 점을 임의의 값으로 바꾼 데이터셋 <br>
> 
> log(λ) : 고유값을 log scale로 변환 <br>
> 
> maxi : 모든 고유값중 변화가 가장 큰 것 선택 <br>
> 
> supm : 해당 m개의 이상치에서 가장 크게 변화하는 경우 <br>
> 
> 1/nmin(...) : 추정값을 무한히 벗어나게 하는 최소 비율

여기서 **비율**을 비교하기 위해 **log**를 사용한다는 것이 가장 큰 차이점이다. <br>

예를들어 5와 50의 비율을 따질때 log5 - log50 으로 log1/10이 되어 대략 2.3을 고르는 식이다.<br>

<br>

k(X)를 R^p의 초평면 위에 놓인 데이터 집합에서의 최대 관측치 개수라고 하자. <br>

여기서 k(X) < h라고 가정하면, raw MCD추정량의 위치와 산포에서 다음이 성립한다.

$$
\epsilon_n^{*}(\hat{\mu}_0; X_n) 
= \epsilon_n^{*}(\hat{\Sigma}_0; X_n) 
= \frac{\min(n - h + 1, \; h - k(X_n))}{n}.
$$

최소한 h개는 이상치로 안바꿔야 추정치가 안 무너진다는 것이고 즉 **h+1이면 무너진다**<br>

그리고 k(X)는 다시말해 데이터가 실제로 차지하는 **최소 차원**이 된다. <br>

예를들어 1개라고 치면 점이 되는것이고 2개라고 치면 평면이 되는 식이다. <br>

**h - k(X)** 는 h에서 최대 몇개까지 뺏을때 **full-rank를 지킬수 있는가**에 대한 식이다. <br>

다시말해, 초평면 밖에 있는 점의 개수이다. 남은게 적으면 det = 0이 되고 즉 비가역이 되버린다. <br>

그리고 이중 작은것이 한계가 된다. <br>

<br>

만약 데이터가 연속 분포로 추출되었다면, 거의 확실하게 k(X) = p가 되고 다음이 성립한다.<br>

여기서 k(X) = p가 되는이유는 연속분포이기 때문에 공간 전체에 샘플들이 흩어져있고 R^p에서는 p개의 변수가 공간을 형성하기 때문이다. <br>

$$
\epsilon_n^{*}(\hat{\mu}_0; X_n) 
= \epsilon_n^{*}(\hat{\Sigma}_0; X_n) 
= \frac{\min(n - h + 1, \; h - p)}{n}.
$$

<br>

<br>

그리고 **[(n + p)/2] <= h <= [(n + p + 1)/2]** 이면 breakdown value는 **(n - p + 2)/2**가 된다. <br>

이유는 단순히 일차함수를 그려보면 이해하기 쉽다. <br>

<img width="800" height="727" alt="image" src="https://github.com/user-attachments/assets/dc9493e8-396a-4cb8-85c2-02dda6df331a" />


<br>

그리고 이는 또한 아핀불변량산포 추정량이 달성할수 있는 **breakdown value의 상한**이다. <br> 이는 일반적인 정규조건(natural regularity conditions) 하에서 성립한다. <br>

$$
n \to \infty \text{일 때,} \quad \lim_{n \to \infty} \epsilon_n^{*} = \min(1 - \alpha, \alpha)
$$

그리고 이는 알파가 0.5일때 최대가 된다. <br>

마지막으로 reweight된 MCD추정량의 평균과 공분산의 breakdown value는 raw MCD의 추정량의 breakdown value보다 작아지지 않는다. <br>

단 가중치 함수 W가 **bounded**이고 큰 d에서는 0이되는 경우에 해당하는데 이는 일정 수준이상의 이상치는 포함시키지 않는다는 것을 의미한다.<br>

<br>

#### 5-3 Influence function

추정량의 influence function은 추정량에 대한 point contamination의 무한소 효과를 측정한다. <br>

즉 추정량의 **민감도**를 측정한다. <br>

이는 모집단 수준에서 정의되고, 따라서 추정량 T의 functional form이 필요하다. <br>

여기서 T는 임의의 분포 F를 parameter space내 값 T(F)로 대응시킨다. <br>

분포 F에서 추정량 T의 영향함수는 다음과 같이 정의한다.

$$
IF(x, T, F) = \lim_{\varepsilon \to 0} \frac{T(F_{\varepsilon}) - T(F)}{\varepsilon}
$$

$$
F_{\varepsilon} = (1 - \varepsilon)F + \varepsilon \Delta_{x}
$$

여기서 델타x가 점질량 x에 있는 **오염된 분포(contaminated distribution)**이다. <br>

수식을 해석하자면 작은비율 입실론만큼 x섞은 분포에서 원래의 분포를 빼서 **변화량**을 구한다고 보면 된다. (도함수의 정의와 유사하게 생겼다) <br>

<br>

raw MCD와 reweighted MCD 추정량의 영향함수는 **유계(bounded)**임이 밝혀졌고, 이는 robust estimator에게 좋은 성질이다. 즉 아무리 이상치가 커도 influence function은 **일정상한**을 넘지 않는다.<br>

즉 이는 추정량이 point contamination에 대해 robust하다는것을 반영한다. <br>

<br>

#### 5-4 Univariate MCD

단변량(Univariate) 데이터의 경우, MCD추정량은 분산이 가장 작은 h-subset의 평균과 분산으로 환원된다. <br>

이것은 인접한 h-subset을 고려하고, 재귀적으로 게산함으로써 O(nlogn)시간 안에 계산될 수 있다. <br>

즉 다변량에서 공분산으로 하던게 단변량에서는 분산으로 되고 즉 분산이 가장 적은 subset찾기로 되는것이다. <br>

h = [n/2] + 1일때, MCD위치 추정량의 breakdown value [(n + 1)/2]/n이고 척도의 추정량은 [n/2]/n이다. <br>

그리고 이 값들은 a**ffine equivariant estimatores**의 **최대치**이다. <br>

여기서 h = [n/2] + 1이 조건을 다시 언급하자면 h를 고를때 정상데이터가 절반 이하면 이상치가 subset을 많이 차지할수 있기때문에 정상데이터는 **절반보다 1개이상**많아야 보장할수있다. <br>

<br>

또한 단변량 MCD 영향함수 또한 **bounded**임이 알려져 있고, 단변량 MCD 위치 추정량은 단변량의 **최소 절단 제곱(LTS, Least Trimmed Squares)** 추정량과 일치하는데 이는 다음과 같다.

$$
\min_{\mu} \sum_{i=1}^{h} \left(r_{\mu}^2\right)_{i:n}
$$

여기서 r^2은 제곱잔차이다. <br>

추가로 LTS와 OLS(최소제곱해, Ordinary Least Squares)의 차이점을 언급하자면, 최소제곱해는 모든 점에 대한 위치고 LTS는 **상위 몇개**만 반영한다 <br>

LTS는 작은 잔차제곱 h만 더해 최소화하기때문에 이상치에 robust하다.

<br>

<br>

### 6. Computation

정확한 MCD추정량을 계산하는 것은 매우 어렵다. <br>

크기가 h인 모든 subset을 평가하기에는 **오래걸리기 때문**이다. <br>

따라서 FT에서 FFT가 나왔듯이, MCD에도 **FAST-MCD알고리즘**이 개발되었다. <br>

이 알고리즘의 핵심요소가 **C-step**이다. <br>

<br>

**Theorem**

데이터 X를 고려하고 H1 (1~n)을 크기가 h인 부분집합이라하자. <br>

H1에 포함된 데이터의 평균과 공분산을 μ1 , Σ1이라 둘때 det(Σ1) != 0이라면, 다음을 relative distace로 정의한다.

$$
d_{1}(i) := \sqrt{(x_i - \hat{\mu}_1)^\top \hat{\Sigma}_1^{-1}(x_i - \hat{\mu}_1)}, 
\quad i = 1, \ldots, n.
$$

그리고 H2를 다음과 같이 정의한다.

$$
\{d_1(i) : i \in H_2\} := \{(d_1)_{1:n}, \ldots, (d_1)_{h:n}\},
$$

이는 **정렬된 거리**들이고, 즉 H2는 작은거리 h개에 해당하는 점들의 집합이다. <br>

그리고 이를 기반으로 새로운 평균과 공분산을 계산하면 다음이 성립한다.

$$
\det(\hat{\Sigma}_2) \leq \det(\hat{\Sigma}_1)
$$

만약 평등일 경우 전과 동일할수있기때문에 **동등**을 포함한다. <br>

만약 det(Σ) > 0이라면 C-step은 항상 **더 작은** 새로운 h-step을 생성한다. <br>

<br>

C-step은 det(Σ) = 0이거나 전과 동일할때까지 반복 할 수있다. <br>

하지만 det(Σ) = 0는 이미 최소조건(거리가 0)이지만, singular하다. 때문에 det=0은 배제한다. <br>

행렬식의 수열은 h-subset개수가 유한해서 반드시 유한한 횟수 안에 끝난다. <br>

하지만 이 결과가 전역최소값이라는 보장은 없다.<br>

n이 커질수록 계산량이 많아져서 이 알고리즘으로도 오래걸리기때문이고 초기집합에 따라서도 다르게 나올수도 있기 때문이다. <br>

때문에, approximate MCD solution은 여러 초기선택을 취하고 각 선택에 대한 C-step을 적용한 후 가장 작은 행렬식을 갖는 해를 유지하는 방식으로 얻을 수 있다.<br>

<br>

초기 H1 부분집합을 구성할때 임의의 **(p+1)-subset** 을 뽑고 계산해서 정렬하면 임의로 h-subset을 봅는것보다 더 나은 초기 부분집합을 만들어 준다. <br>

이는 반으로 가까운 점 h 선택하는것이 이상치가 포함되지 않을 확률이 높기 때문이다. <br>

여기서 p+1인 이유는 앞에서 언급했듯이 full-rank를 취해야 하는 최소조건이기 때문이다. <br>

직관적으로 최소개를 뽑으면 이상치가 **최소개** 나오는것은 직관적으로 알 수 있다.<br>

실제로는 단 2번의 C-step후에 **전역최소** 에 수렴하는 많은 실행들이 이미 작은 det를 가져서 각 초기 집합에서 단 2번의 과정만 거친후 얻어진 10개의 subset중 가장 작은 것을 선택하는 방식으로 줄인다.<br>

<br>

### APPLICATION & CONCLUSION

많은 다변량 통계기법들이 공분산 추정에 의존하는데 MCD추정량은 robust한 다변량 기법을 구성하는데 매우 적합하다.<br>

예를들어 mcd의 회귀(regression) 아날로그는 LTS estimator인데, 이는 가장 작은 h개의 제곱 잔차의 합을 최소화한다. <br>

즉, LTS 추정은 h-subset에 대해 최소제곱 적합을 수행한것이고 가장 거리가 작은 경우이다. <br>

FAST-LST는 FAST-MCD와 유사한 기법을 사용한다. <br>

이외에서 MCD응용은 매우 다양하고 최근 응용만 봐도 , 금융, 계량경제, 의학 ,품질관리 등 매우 다양하다.
