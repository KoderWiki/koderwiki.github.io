---
date: 2025-12-28 23:04:00
layout: post
title: resilient Multiple Choice Learning
subtitle: 'rMCL'
description: >-
  rMCL에 대해 알아보자
image: >-
  https://koderwiki.github.io/assets/img/0.post/linear/rmcl.png
optimized_image: >-
  https://koderwiki.github.io/assets/img/0.post/linear/rmcl.png
category: Machine Learning
tags:
    - rMCL
    - WTA
    - Distribution estimation
    - Probability Theory
    - Machine Learning
    - blog
author: geonu.Ko
paginate: true
use_math : true
---

## Resilient Multiple Choice Learning

### Introduction

머신러닝 모델은 일반적으로 주어진 입력에 대해 단일한 예측값을 출력하도록 학습되지만,

데이터의 본질적 특성이나 다양한 불확실성 요인으로 인해 조건부 출력 분포가 Multimodal 형태를 띄는 경우가 많다. <br>

이러한 경우 조건부 확률분포의 평균은 저밀도 영역에 위치할 수 있으며, 단일 예측 대신 여러개의 가설을 동시에 예측하는 것이 더 바람직할 수 있다. <br>

이러한 맥락에서 Multiple Choice Learning(MCL)은 각 head마다 하나의 예측가설을 생성할 수 있어 실용적인 해결책으로 제안되고 있다. 지도학습에서는 현재 입력 sample에 대해 가장 우수한 head에만 gradient를 계산하는 Winner-Takes-All (WTA) 방식은 각 head가 출력 공간의 서로다른 영역에 특화되도록 유도한다. <br>

하지만 MCL은 hyppotheses collapse와 overconfidence라는 중대한 문제점을 갖고있어 이를 완화하기 위해 제안된 resilient Multiple Choice Learning (rMCL) 이 제안되었다. <br>

### Multiple Choice Learning

하나의 입력 x에 대해 여러개의 후보 예측을 동시에 학습하고 정답과 가장 잘 맞는 하나만 업데이트하는 학습 방식이다 <br>

MCL의 목표는 하나의 입력에 대해 여러 개의 출력을 동시에 학습함으로써, 단일 예측으로는 표현하기 어려운 조건부 출력 분포를 붕괴없이 근사하는 것이다 <br>

**Model Uncertainty** <br>

모델 불확실성은 주로 학습데이터나 부족하거나 입력 공간이 충분히 관측되지 않았을때 나타날 수 있다 <br>

**Data Uncertainty** <br>

데이터 불확실성은 관측 노이즈, 오차, 또는 생성과정에서의 내재적 확률성으로부터 발생할 수 있으며, 데이터 수가 증가해도 근본적으로 사라지지 않는다 <br>

<br>

다음과 같이 입력 x 하나에 여러개의 예측가설이 존재할때,

$$
\hat y_1(x), \hat y_2(x), ....,\hat y_K(x)
$$

K개의 예측중에서 정답에 가장 가까운 예측하나만 선택해서 손실을 계산한다

$$
L(x,y) = \min_{k=1,...,K} l(y,\hat y_k(x)
$$

하지만 기존 MCL에는 중대한 2가지의 문제점이 존재한다

**Hypothesis collapse** <br>

한 head가 주도권을 잡으면 다른 head는 선택되지 않게되고, 한개의 head만 계속해서 업데이트되어 다른 head들은 훈련되지 않아 의미없는 출력이 생성될 수 있다 <br>

**Overconfidence** <br>

확률 개념이 없어, rare mode도 하나의 대표 point가 되어 학습될 수 있고 이는 곧 분포를 왜곡 시킬 수 있다 <br>

<br>

### Fixing the overconfidence issue in Multiple Choice Learning